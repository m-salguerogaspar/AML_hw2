{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "776c3c2b",
      "metadata": {
        "id": "776c3c2b"
      },
      "source": [
        "# ASSIGNMENT 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b629ed8",
      "metadata": {
        "id": "6b629ed8"
      },
      "outputs": [],
      "source": [
        "#!pip install torchsummary\n",
        "#pip install interpret\n",
        "#!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c5c21a",
      "metadata": {
        "id": "17c5c21a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from interpret.glassbox import ExplainableBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0189e0a4",
      "metadata": {
        "id": "0189e0a4"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xuPHIn5Vg5LW",
      "metadata": {
        "id": "xuPHIn5Vg5LW"
      },
      "outputs": [],
      "source": [
        "identity  = pd.read_csv('data_identity.csv')\n",
        "transaction = pd.read_csv('data_transaction.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496da53d",
      "metadata": {
        "id": "496da53d"
      },
      "outputs": [],
      "source": [
        "joined_table = pd.merge(identity,transaction, on = 'TransactionID')\n",
        "joined_table = joined_table.drop([col for col in joined_table if joined_table[col].dtype  == 'object'], axis = 1)\n",
        "joined_table = joined_table.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d34c801",
      "metadata": {
        "id": "5d34c801"
      },
      "outputs": [],
      "source": [
        "transaction = None\n",
        "identity = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc27f63",
      "metadata": {
        "id": "8fc27f63"
      },
      "outputs": [],
      "source": [
        "k = 1\n",
        "empty_columns = []\n",
        "for elem in joined_table.columns:\n",
        "    if joined_table[elem].isnull().values.all():\n",
        "        empty_columns.append(elem)\n",
        "joined_table = joined_table.drop(empty_columns, axis = 1)\n",
        "\n",
        "X = joined_table.fillna(0.0)\n",
        "X = X.reset_index().drop(['index'] , axis = 1)\n",
        "\n",
        "indicator = []\n",
        "for ind in range(joined_table.shape[0]):\n",
        "    indicator.append([int(not elem) for elem in joined_table.iloc[ind].isna()])\n",
        "indicator = np.array(indicator)\n",
        "indicator = pd.DataFrame(indicator, columns = joined_table.columns, dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120b2379",
      "metadata": {
        "id": "120b2379",
        "outputId": "ef7dbde9-ce05-4d2f-a6fb-7476f2730acc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>...</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144228</th>\n",
              "      <td>3577521.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>145955.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144229</th>\n",
              "      <td>3577526.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>172059.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144230</th>\n",
              "      <td>3577529.0</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>632381.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-36.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144231</th>\n",
              "      <td>3577531.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>55528.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144232</th>\n",
              "      <td>3577534.0</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>339406.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>144233 rows × 390 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        TransactionID  id_01     id_02  id_03  id_04  id_05  id_06  id_07  \\\n",
              "0           2987004.0    0.0   70787.0    NaN    NaN    NaN    NaN    NaN   \n",
              "1           2987008.0   -5.0   98945.0    NaN    NaN    0.0   -5.0    NaN   \n",
              "2           2987010.0   -5.0  191631.0    0.0    0.0    0.0    0.0    NaN   \n",
              "3           2987011.0   -5.0  221832.0    NaN    NaN    0.0   -6.0    NaN   \n",
              "4           2987016.0    0.0    7460.0    0.0    0.0    1.0    0.0    NaN   \n",
              "...               ...    ...       ...    ...    ...    ...    ...    ...   \n",
              "144228      3577521.0  -15.0  145955.0    0.0    0.0    0.0    0.0    NaN   \n",
              "144229      3577526.0   -5.0  172059.0    NaN    NaN    1.0   -5.0    NaN   \n",
              "144230      3577529.0  -20.0  632381.0    NaN    NaN   -1.0  -36.0    NaN   \n",
              "144231      3577531.0   -5.0   55528.0    0.0    0.0    0.0   -7.0    NaN   \n",
              "144232      3577534.0  -45.0  339406.0    NaN    NaN  -10.0 -100.0    NaN   \n",
              "\n",
              "        id_08  id_09  ...  V330  V331  V332  V333  V334  V335  V336  V337  \\\n",
              "0         NaN    NaN  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
              "1         NaN    NaN  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
              "2         NaN    0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
              "3         NaN    NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
              "4         NaN    0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
              "...       ...    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
              "144228    NaN    0.0  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
              "144229    NaN    NaN  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
              "144230    NaN    NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
              "144231    NaN    0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
              "144232    NaN    NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
              "\n",
              "        V338  V339  \n",
              "0        0.0   0.0  \n",
              "1        0.0   0.0  \n",
              "2        NaN   NaN  \n",
              "3        NaN   NaN  \n",
              "4        0.0   0.0  \n",
              "...      ...   ...  \n",
              "144228   NaN   NaN  \n",
              "144229   0.0   0.0  \n",
              "144230   NaN   NaN  \n",
              "144231   0.0   0.0  \n",
              "144232   NaN   NaN  \n",
              "\n",
              "[144233 rows x 390 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joined_table.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac16542",
      "metadata": {
        "id": "0ac16542"
      },
      "outputs": [],
      "source": [
        "# Mean strategy for imputing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc698b1",
      "metadata": {
        "id": "dbc698b1"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputer.fit_transform(joined_table).shape\n",
        "mask = pd.DataFrame(imputer.fit_transform(joined_table), columns=joined_table.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pOQSrc_RgIWt",
      "metadata": {
        "id": "pOQSrc_RgIWt"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "mask = scaler.fit_transform(mask)\n",
        "\n",
        "AE_input = np.array([(m,i) for m, i in zip(np.array(mask), np.array(indicator))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5927b66d",
      "metadata": {
        "id": "5927b66d",
        "outputId": "879d7653-1a00-4060-aaa0-b834298865a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(144233, 390) (144233, 390) (144233, 390)\n"
          ]
        }
      ],
      "source": [
        "print(mask.shape, indicator.shape, joined_table.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2e181b",
      "metadata": {
        "id": "fb2e181b"
      },
      "source": [
        "## APPROACHES FOR MEAN AND MODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29af5eb5",
      "metadata": {
        "id": "29af5eb5"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "mean_approach = pd.DataFrame(mean_imputer.fit_transform(joined_table), columns=joined_table.columns)\n",
        "mean_approach = pd.DataFrame(scaler.fit_transform(mean_approach), columns=joined_table.columns)\n",
        "mean_approach.to_csv(r'mean_approach', index=False)\n",
        "\n",
        "\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "mode_approach = pd.DataFrame(mode_imputer.fit_transform(joined_table), columns=joined_table.columns)\n",
        "mode_approach = pd.DataFrame(scaler.fit_transform(mode_approach), columns=joined_table.columns)\n",
        "mode_approach.to_csv(r'mode_approach', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fb0771",
      "metadata": {
        "id": "33fb0771"
      },
      "source": [
        "### 1.2 Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680c97bf",
      "metadata": {
        "id": "680c97bf",
        "outputId": "890a77a5-3aa4-4f07-dd1f-36fe8eb93ac0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aba7c97",
      "metadata": {
        "id": "0aba7c97"
      },
      "source": [
        "#### 1.2.1 Undercomplete Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e78e2636",
      "metadata": {
        "id": "e78e2636"
      },
      "outputs": [],
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, latent_dim):\n",
        "      super(autoencoder, self).__init__()\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(input_size, input_size//2),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.Linear(input_size//2, input_size//3),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.Linear(input_size//3, input_size//4),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.Linear(input_size//4, latent_dim),\n",
        "          nn.LeakyReLU()\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(latent_dim, input_size//4),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(input_size//4, input_size//3),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(input_size//3, input_size//2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(input_size//2, input_size),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "      self.encoder.apply(self.__init_weights)\n",
        "      self.decoder.apply(self.__init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.encoder(x)\n",
        "      x = self.decoder(x)\n",
        "      return x\n",
        "        \n",
        "    def encode(self,input):\n",
        "      return self.encoder(input)\n",
        "    \n",
        "    def __init_weights(self,m):\n",
        "      if type(m) == nn.Linear:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd895aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbd895aa",
        "outputId": "98ef3399-35a7-42bf-b574-50a906d98241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=390, out_features=195, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=195, out_features=130, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Linear(in_features=130, out_features=97, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Linear(in_features=97, out_features=50, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=97, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=97, out_features=130, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=130, out_features=195, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=195, out_features=390, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "AE = autoencoder(AE_input.shape[-1],50).to(device)\n",
        "print(AE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da851c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "8da851c6",
        "outputId": "33825fed-2748-46fc-eba0-adeba306b1fa",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/25], loss:339.5121\n",
            "epoch [2/25], loss:243.6956\n",
            "epoch [3/25], loss:223.0826\n",
            "epoch [4/25], loss:209.5512\n",
            "epoch [5/25], loss:196.1432\n",
            "epoch [6/25], loss:186.8426\n",
            "epoch [7/25], loss:171.3998\n",
            "epoch [8/25], loss:155.1210\n",
            "epoch [9/25], loss:148.6340\n",
            "epoch [10/25], loss:142.1903\n",
            "epoch [11/25], loss:137.6241\n",
            "epoch [12/25], loss:134.4864\n",
            "epoch [13/25], loss:133.4642\n",
            "epoch [14/25], loss:131.6101\n",
            "epoch [15/25], loss:126.8570\n",
            "epoch [16/25], loss:123.4985\n",
            "epoch [17/25], loss:121.8739\n",
            "epoch [18/25], loss:120.9497\n",
            "epoch [19/25], loss:123.0770\n",
            "epoch [20/25], loss:119.4522\n",
            "epoch [21/25], loss:120.6210\n",
            "epoch [22/25], loss:119.0375\n",
            "epoch [23/25], loss:121.7632\n",
            "epoch [24/25], loss:119.3797\n",
            "epoch [25/25], loss:117.6711\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "num_epochs = 25\n",
        "criterion = nn.L1Loss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(AE.parameters(),lr=learning_rate)\n",
        "data_loader = DataLoader(AE_input, batch_size=64,shuffle=True)\n",
        "\n",
        "X_reconstuct = []\n",
        "Original_X = []\n",
        "I_X = []\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = list()\n",
        "  for X_batch in data_loader:\n",
        "    X, indic = X_batch[:, 0, :].to(device), X_batch[:, 1, :].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    X_hat = AE(X)\n",
        "    loss = criterion(torch.mul(indic, X), torch.mul(indic, X_hat))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss.append(loss.item())\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    X_reconstuct.append(AE(X))\n",
        "    Original_X.append(X)\n",
        "    I_X.append(1 - indic)\n",
        "    \n",
        "  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, np.mean(epoch_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c92dbf1",
      "metadata": {
        "id": "6c92dbf1",
        "outputId": "db84f91a-eac0-4bad-db0b-3db34b156a95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(144233, 390)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_loader = DataLoader(AE_input, batch_size=64,shuffle=False)\n",
        "\n",
        "X_undercomplete = []\n",
        "for X_batch in data_loader:\n",
        "  for X in X_batch[:, 0, :]:\n",
        "    with torch.no_grad():\n",
        "        AE.eval()\n",
        "        X_undercomplete.append(AE(X.float()).numpy())\n",
        "  \n",
        "np.array(X_undercomplete).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8679c5d6",
      "metadata": {
        "id": "8679c5d6"
      },
      "outputs": [],
      "source": [
        "pd_undercomplete = pd.DataFrame(X_undercomplete, columns=joined_table.columns)\n",
        "pd_undercomplete.to_csv(r'pd_undercomplete', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd12c45",
      "metadata": {
        "id": "4cd12c45"
      },
      "source": [
        "# Regularized autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KNH58fOoNZGp",
      "metadata": {
        "id": "KNH58fOoNZGp",
        "outputId": "846a47f1-ef05-44cd-d30a-f70ec274735a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=390, out_features=195, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=195, out_features=130, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Linear(in_features=130, out_features=97, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Linear(in_features=97, out_features=50, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=97, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=97, out_features=130, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=130, out_features=195, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=195, out_features=390, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(AE)\n",
        "\n",
        "model_children = list(AE.children())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796c028d",
      "metadata": {
        "id": "796c028d"
      },
      "outputs": [],
      "source": [
        "# DEFINING REGULARIZATION LOSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HztgK7adQeyD",
      "metadata": {
        "id": "HztgK7adQeyD"
      },
      "outputs": [],
      "source": [
        "def sparse_loss(autoencoder, data):\n",
        "    loss = 0\n",
        "    values = data\n",
        "    for i in range(len(model_children)):\n",
        "        values = F.relu((model_children[i](values)))\n",
        "        loss += torch.mean(torch.abs(values))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee504db",
      "metadata": {
        "id": "eee504db",
        "scrolled": true,
        "outputId": "ae42c916-404a-4e41-fc61-be3961f4b7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/25], loss:118.7757\n",
            "epoch [2/25], loss:117.4605\n",
            "epoch [3/25], loss:118.1848\n",
            "epoch [4/25], loss:115.2087\n",
            "epoch [5/25], loss:114.9248\n",
            "epoch [6/25], loss:116.0172\n",
            "epoch [7/25], loss:112.6678\n",
            "epoch [8/25], loss:112.6040\n",
            "epoch [9/25], loss:112.1812\n",
            "epoch [10/25], loss:111.7484\n",
            "epoch [11/25], loss:120.8103\n",
            "epoch [12/25], loss:117.7912\n",
            "epoch [13/25], loss:114.2734\n",
            "epoch [14/25], loss:114.5020\n",
            "epoch [15/25], loss:112.7838\n",
            "epoch [16/25], loss:111.3956\n",
            "epoch [17/25], loss:111.0736\n",
            "epoch [18/25], loss:110.8399\n",
            "epoch [19/25], loss:110.9074\n",
            "epoch [20/25], loss:110.6177\n",
            "epoch [21/25], loss:111.0417\n",
            "epoch [22/25], loss:110.3374\n",
            "epoch [23/25], loss:113.8784\n",
            "epoch [24/25], loss:111.2613\n",
            "epoch [25/25], loss:111.9568\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "num_epochs = 25\n",
        "criterion = nn.L1Loss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(AE.parameters(),lr=learning_rate)\n",
        "data_loader = DataLoader(AE_input, batch_size=64,shuffle=True)\n",
        "reg_param = 1.0\n",
        "\n",
        "\n",
        "X_reconstuct = []\n",
        "Original_X = []\n",
        "I_X = []\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = list()\n",
        "  for X_batch in data_loader:\n",
        "    X, indic = X_batch[:, 0, :].to(device), X_batch[:, 1, :].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    X_hat = AE(X)\n",
        "    loss = criterion(torch.mul(indic, X), torch.mul(indic, X_hat))\n",
        "    l1_loss = sparse_loss(AE, X)\n",
        "    loss = loss + reg_param * l1_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss.append(loss.item())\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    X_reconstuct.append(AE(X))\n",
        "    Original_X.append(X)\n",
        "    I_X.append(1 - indic)\n",
        "  # log\n",
        "  print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, np.mean(epoch_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1tg1p2ipGjdD",
      "metadata": {
        "id": "1tg1p2ipGjdD",
        "outputId": "bbd0bf82-96c5-46d6-f18c-cd4d72bc7985"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(144233, 390)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_loader = DataLoader(AE_input, batch_size=64,shuffle=False)\n",
        "\n",
        "X_regularized = []\n",
        "for X_batch in data_loader:\n",
        "  for X in X_batch[:, 0, :]:\n",
        "    with torch.no_grad():\n",
        "        AE.eval()\n",
        "        X_regularized.append(AE(X.float()).numpy())\n",
        "  \n",
        "np.array(X_regularized).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2109acfa",
      "metadata": {
        "id": "2109acfa"
      },
      "outputs": [],
      "source": [
        "pd_regularized = pd.DataFrame(X_regularized, columns=joined_table.columns)\n",
        "pd_regularized.to_csv(r'pd_regularized', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149b74a9",
      "metadata": {
        "id": "149b74a9"
      },
      "source": [
        "# VARIATIONAL AUTOENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6691510a",
      "metadata": {
        "id": "6691510a"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size=390, h_dim=400, z_dim=50):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, input_size)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return F.sigmoid(self.fc5(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x[0]*x[1])\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334c9187",
      "metadata": {
        "id": "334c9187",
        "outputId": "913a9987-1247-475d-a4fb-d39170720318"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mauricio\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/25], loss:461.4106\n",
            "epoch [2/25], loss:416.9871\n",
            "epoch [3/25], loss:396.2362\n",
            "epoch [4/25], loss:383.5941\n",
            "epoch [5/25], loss:374.7254\n",
            "epoch [6/25], loss:368.2826\n",
            "epoch [7/25], loss:363.2577\n",
            "epoch [8/25], loss:359.3211\n",
            "epoch [9/25], loss:356.1127\n",
            "epoch [10/25], loss:353.4022\n",
            "epoch [11/25], loss:350.9548\n",
            "epoch [12/25], loss:348.9056\n",
            "epoch [13/25], loss:347.0969\n",
            "epoch [14/25], loss:345.5533\n",
            "epoch [15/25], loss:344.0623\n",
            "epoch [16/25], loss:343.0020\n",
            "epoch [17/25], loss:342.0407\n",
            "epoch [18/25], loss:341.2294\n",
            "epoch [19/25], loss:340.5282\n",
            "epoch [20/25], loss:339.8868\n",
            "epoch [21/25], loss:339.2862\n",
            "epoch [22/25], loss:338.7240\n",
            "epoch [23/25], loss:338.1885\n",
            "epoch [24/25], loss:337.6669\n",
            "epoch [25/25], loss:337.1725\n"
          ]
        }
      ],
      "source": [
        "batchSize = 64 \n",
        "learning_rate = 0.05 \n",
        "num_epochs = 25 \n",
        "X_reconstuct = []\n",
        "Original_X = []\n",
        "I_X = []\n",
        "epoch_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for  X_batch in data_loader:\n",
        "        x, indic = X_batch[:, 0, :].to(device), X_batch[:, 1, :].to(device)\n",
        "        x_reconst, mu, log_var = model(x.float())\n",
        "        reconst_loss = criterion(indic * x, indic * x_reconst)\n",
        "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        loss = reconst_loss + kl_div\n",
        "        epoch_loss.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, np.mean(epoch_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c1e7a97",
      "metadata": {
        "id": "7c1e7a97",
        "outputId": "df83a8c2-fcb8-475b-a411-cfe13b7b529b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(144233, 390)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_loader = DataLoader(AE_input, batch_size=64,shuffle=False)\n",
        "\n",
        "X_variational = []\n",
        "for X_batch in data_loader:\n",
        "  for X in X_batch[:, 0, :]:\n",
        "    with torch.no_grad():\n",
        "        AE.eval()\n",
        "        X_variational.append(AE(X.float()).numpy())\n",
        "  \n",
        "np.array(X_variational).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a0f0e9",
      "metadata": {
        "id": "83a0f0e9"
      },
      "outputs": [],
      "source": [
        "pd_variational = pd.DataFrame(X_variational, columns=joined_table.columns)\n",
        "pd_variational.to_csv(r'pd_variational', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96154f97",
      "metadata": {
        "id": "96154f97"
      },
      "source": [
        "# Task 2\n",
        "### 2.1 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install interpret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1nGujr8TjomN",
        "outputId": "8a576197-4821-438e-a354-804816a3d51f"
      },
      "id": "1nGujr8TjomN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting interpret\n",
            "  Downloading interpret-0.2.7-py3-none-any.whl (1.4 kB)\n",
            "Collecting interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7\n",
            "  Downloading interpret_core-0.2.7-py3-none-any.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 13.0 MB/s \n",
            "\u001b[?25hCollecting treeinterpreter>=0.2.2\n",
            "  Downloading treeinterpreter-0.2.3-py2.py3-none-any.whl (6.0 kB)\n",
            "Collecting psutil>=5.6.2\n",
            "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting shap>=0.28.5\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 67.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.3.4)\n",
            "Collecting lime>=0.1.1.33\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.0.2)\n",
            "Collecting skope-rules>=1.0.1\n",
            "  Downloading skope_rules-1.0.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (5.5.0)\n",
            "Collecting ipykernel>=5.1.0\n",
            "  Downloading ipykernel-6.13.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting ipython>=7.4.0\n",
            "  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting SALib>=1.3.3\n",
            "  Downloading SALib-1.4.5-py2.py3-none-any.whl (756 kB)\n",
            "\u001b[K     |████████████████████████████████| 756 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting dash>=1.0.0\n",
            "  Downloading dash-2.4.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 56.5 MB/s \n",
            "\u001b[?25hCollecting dash-table>=4.1.0\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting dash-cytoscape>=0.1.1\n",
            "  Downloading dash_cytoscape-0.3.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 56.1 MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.23.0)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.1.4)\n",
            "Collecting dash-html-components==2.0.0\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting flask-compress\n",
            "  Downloading Flask_Compress-1.12-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (7.1.2)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (57.4.0)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-7.3.1-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (5.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.5.5)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.1.3)\n",
            "Collecting tornado>=6.1\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (21.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.4)\n",
            "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.8.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.64.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.18.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2022.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=3.8.1->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=3.8.1->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (8.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (3.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.37.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.11.3)\n",
            "Collecting pathos\n",
            "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->lime>=0.1.1.33->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (4.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.1->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (3.1.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (1.3.0)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 72.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (3.8.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap>=0.28.5->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.34.0)\n",
            "Collecting ppft>=1.6.6.4\n",
            "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting pox>=0.3.0\n",
            "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos->SALib>=1.3.3->interpret-core[dash,debug,decisiontree,ebm,lime,linear,notebook,plotly,required,sensitivity,shap,skoperules,treeinterpreter]>=0.2.7->interpret) (0.70.12.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=874d01f445e00c06a896d21ed0f93b8b0b4c374ffc1774c646fea14e1402b44e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "Successfully built lime\n",
            "Installing collected packages: brotli, tornado, prompt-toolkit, ppft, pox, flask-compress, dash-table, dash-html-components, dash-core-components, zope.interface, zope.event, slicer, psutil, pathos, jupyter-client, ipython, dash, treeinterpreter, skope-rules, shap, SALib, lime, ipykernel, interpret-core, gevent, dash-cytoscape, interpret\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed SALib-1.4.5 brotli-1.0.9 dash-2.4.1 dash-core-components-2.0.0 dash-cytoscape-0.3.0 dash-html-components-2.0.0 dash-table-5.0.0 flask-compress-1.12 gevent-21.12.0 interpret-0.2.7 interpret-core-0.2.7 ipykernel-6.13.0 ipython-7.33.0 jupyter-client-7.3.1 lime-0.2.0.1 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 prompt-toolkit-3.0.29 psutil-5.9.0 shap-0.40.0 skope-rules-1.0.1 slicer-0.0.7 tornado-6.1 treeinterpreter-0.2.3 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "jupyter_client",
                  "prompt_toolkit",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fbd86dfc",
      "metadata": {
        "id": "fbd86dfc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms, utils\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from interpret.glassbox import ExplainableBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xpQVB1lLa1Q",
      "metadata": {
        "id": "0xpQVB1lLa1Q"
      },
      "outputs": [],
      "source": [
        "train_path = 'UNSW_NB15_training-set.csv'\n",
        "trainset  = pd.read_csv(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E5PXEELZNGkB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "E5PXEELZNGkB",
        "outputId": "182be20c-e973-4d5e-9a01-8d310422fa7f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>...</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "      <th>attack_cat</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>496</td>\n",
              "      <td>0</td>\n",
              "      <td>90909.0902</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1762</td>\n",
              "      <td>0</td>\n",
              "      <td>125000.0003</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>200000.0051</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>900</td>\n",
              "      <td>0</td>\n",
              "      <td>166666.6608</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>0</td>\n",
              "      <td>100000.0025</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
              "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
              "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
              "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
              "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
              "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
              "\n",
              "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
              "0   90909.0902  ...                 1               2             0   \n",
              "1  125000.0003  ...                 1               2             0   \n",
              "2  200000.0051  ...                 1               3             0   \n",
              "3  166666.6608  ...                 1               3             0   \n",
              "4  100000.0025  ...                 1               3             0   \n",
              "\n",
              "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
              "0           0                 0           1           2                0   \n",
              "1           0                 0           1           2                0   \n",
              "2           0                 0           1           3                0   \n",
              "3           0                 0           2           3                0   \n",
              "4           0                 0           2           3                0   \n",
              "\n",
              "   attack_cat  label  \n",
              "0      Normal      0  \n",
              "1      Normal      0  \n",
              "2      Normal      0  \n",
              "3      Normal      0  \n",
              "4      Normal      0  \n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gIrxqhF4NH5V",
      "metadata": {
        "id": "gIrxqhF4NH5V",
        "outputId": "e038b22b-85d6-4fbb-fdb9-2f8e182c6750"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>djit</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>496</td>\n",
              "      <td>0</td>\n",
              "      <td>90909.0902</td>\n",
              "      <td>180363632.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>248</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>1762</td>\n",
              "      <td>0</td>\n",
              "      <td>125000.0003</td>\n",
              "      <td>881000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>881</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>200000.0051</td>\n",
              "      <td>854400000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>534</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>900</td>\n",
              "      <td>0</td>\n",
              "      <td>166666.6608</td>\n",
              "      <td>600000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>450</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>2126</td>\n",
              "      <td>0</td>\n",
              "      <td>100000.0025</td>\n",
              "      <td>850400000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1063</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       dur  sbytes  dbytes         rate        sload  dload  sinpkt  \\\n",
              "0   1  0.000011     496       0   90909.0902  180363632.0    0.0   0.011   \n",
              "1   2  0.000008    1762       0  125000.0003  881000000.0    0.0   0.008   \n",
              "2   3  0.000005    1068       0  200000.0051  854400000.0    0.0   0.005   \n",
              "3   4  0.000006     900       0  166666.6608  600000000.0    0.0   0.006   \n",
              "4   5  0.000010    2126       0  100000.0025  850400000.0    0.0   0.010   \n",
              "\n",
              "   dinpkt  sjit  djit  stcpb  dtcpb  tcprtt  synack  ackdat  smean  dmean  \\\n",
              "0     0.0   0.0   0.0      0      0     0.0     0.0     0.0    248      0   \n",
              "1     0.0   0.0   0.0      0      0     0.0     0.0     0.0    881      0   \n",
              "2     0.0   0.0   0.0      0      0     0.0     0.0     0.0    534      0   \n",
              "3     0.0   0.0   0.0      0      0     0.0     0.0     0.0    450      0   \n",
              "4     0.0   0.0   0.0      0      0     0.0     0.0     0.0   1063      0   \n",
              "\n",
              "   response_body_len  label  \n",
              "0                  0      0  \n",
              "1                  0      0  \n",
              "2                  0      0  \n",
              "3                  0      0  \n",
              "4                  0      0  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columnas = list()\n",
        "\n",
        "for col in list(trainset.columns):\n",
        "    valuesinside = len(list(trainset[col].unique()))/len(trainset[col])\n",
        "    if valuesinside >= 0.01:\n",
        "       columnas.append(col)\n",
        "\n",
        "columnas = columnas + [\"attack_cat\"] + [\"label\"]\n",
        "trainset = trainset[columnas]\n",
        "cattrain = trainset[\"attack_cat\"]\n",
        "trainset.drop(\"attack_cat\", axis=1, inplace=True)\n",
        "trainset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b043563",
      "metadata": {
        "id": "7b043563"
      },
      "outputs": [],
      "source": [
        "test_path = 'UNSW_NB15_testing-set.csv'\n",
        "testset  = pd.read_csv(test_path)\n",
        "testset = testset[columnas]\n",
        "cattest = testset[\"attack_cat\"]\n",
        "testset.drop(\"attack_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613a9c3b",
      "metadata": {
        "id": "613a9c3b",
        "outputId": "4542255b-9749-40f2-8680-264c708f005a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>djit</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.121478</td>\n",
              "      <td>258</td>\n",
              "      <td>172</td>\n",
              "      <td>74.087490</td>\n",
              "      <td>14158.942380</td>\n",
              "      <td>8495.365234</td>\n",
              "      <td>24.295600</td>\n",
              "      <td>8.375000</td>\n",
              "      <td>30.177547</td>\n",
              "      <td>11.830604</td>\n",
              "      <td>621772692</td>\n",
              "      <td>2202533631</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.649902</td>\n",
              "      <td>734</td>\n",
              "      <td>42014</td>\n",
              "      <td>78.473372</td>\n",
              "      <td>8395.112305</td>\n",
              "      <td>503571.312500</td>\n",
              "      <td>49.915000</td>\n",
              "      <td>15.432865</td>\n",
              "      <td>61.426934</td>\n",
              "      <td>1387.778330</td>\n",
              "      <td>1417884146</td>\n",
              "      <td>3077387971</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>1106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.623129</td>\n",
              "      <td>364</td>\n",
              "      <td>13186</td>\n",
              "      <td>14.170161</td>\n",
              "      <td>1572.271851</td>\n",
              "      <td>60929.230470</td>\n",
              "      <td>231.875571</td>\n",
              "      <td>102.737203</td>\n",
              "      <td>17179.586860</td>\n",
              "      <td>11420.926230</td>\n",
              "      <td>2116150707</td>\n",
              "      <td>2963114973</td>\n",
              "      <td>0.111897</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0.050439</td>\n",
              "      <td>46</td>\n",
              "      <td>824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.681642</td>\n",
              "      <td>628</td>\n",
              "      <td>770</td>\n",
              "      <td>13.677108</td>\n",
              "      <td>2740.178955</td>\n",
              "      <td>3358.622070</td>\n",
              "      <td>152.876547</td>\n",
              "      <td>90.235726</td>\n",
              "      <td>259.080172</td>\n",
              "      <td>4991.784669</td>\n",
              "      <td>1107119177</td>\n",
              "      <td>1047442890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.449454</td>\n",
              "      <td>534</td>\n",
              "      <td>268</td>\n",
              "      <td>33.373826</td>\n",
              "      <td>8561.499023</td>\n",
              "      <td>3987.059814</td>\n",
              "      <td>47.750333</td>\n",
              "      <td>75.659602</td>\n",
              "      <td>2415.837634</td>\n",
              "      <td>115.807000</td>\n",
              "      <td>2436137549</td>\n",
              "      <td>1977154190</td>\n",
              "      <td>0.128381</td>\n",
              "      <td>0.071147</td>\n",
              "      <td>0.057234</td>\n",
              "      <td>53</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       dur  sbytes  dbytes       rate         sload          dload  \\\n",
              "0   1  0.121478     258     172  74.087490  14158.942380    8495.365234   \n",
              "1   2  0.649902     734   42014  78.473372   8395.112305  503571.312500   \n",
              "2   3  1.623129     364   13186  14.170161   1572.271851   60929.230470   \n",
              "3   4  1.681642     628     770  13.677108   2740.178955    3358.622070   \n",
              "4   5  0.449454     534     268  33.373826   8561.499023    3987.059814   \n",
              "\n",
              "       sinpkt      dinpkt          sjit          djit       stcpb       dtcpb  \\\n",
              "0   24.295600    8.375000     30.177547     11.830604   621772692  2202533631   \n",
              "1   49.915000   15.432865     61.426934   1387.778330  1417884146  3077387971   \n",
              "2  231.875571  102.737203  17179.586860  11420.926230  2116150707  2963114973   \n",
              "3  152.876547   90.235726    259.080172   4991.784669  1107119177  1047442890   \n",
              "4   47.750333   75.659602   2415.837634    115.807000  2436137549  1977154190   \n",
              "\n",
              "     tcprtt    synack    ackdat  smean  dmean  response_body_len  label  \n",
              "0  0.000000  0.000000  0.000000     43     43                  0      0  \n",
              "1  0.000000  0.000000  0.000000     52   1106                  0      0  \n",
              "2  0.111897  0.061458  0.050439     46    824                  0      0  \n",
              "3  0.000000  0.000000  0.000000     52     64                  0      0  \n",
              "4  0.128381  0.071147  0.057234     53     45                  0      0  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81406788",
      "metadata": {
        "id": "81406788"
      },
      "outputs": [],
      "source": [
        "columnas.remove(\"attack_cat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187831a7",
      "metadata": {
        "id": "187831a7"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "trainset = pd.DataFrame(scaler.fit_transform(trainset[columnas]), columns=columnas)\n",
        "testset = pd.DataFrame(scaler.transform(testset[columnas]), columns=columnas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e4a9fe",
      "metadata": {
        "id": "95e4a9fe"
      },
      "outputs": [],
      "source": [
        "trainset[\"attack_cat\"] = cattrain\n",
        "testset[\"attack_cat\"] = cattest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09c91bd",
      "metadata": {
        "id": "e09c91bd",
        "outputId": "f14ef285-b0d4-4de1-890f-625c34c29db2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>...</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>label</th>\n",
              "      <th>attack_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.024634e-03</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>2.687726e-06</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>4.048592e-04</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.144768</td>\n",
              "      <td>0.512828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012838</td>\n",
              "      <td>0.028667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.083170e-02</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>1.593605e-06</td>\n",
              "      <td>0.024186</td>\n",
              "      <td>8.317781e-04</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>...</td>\n",
              "      <td>0.330128</td>\n",
              "      <td>0.716525</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018919</td>\n",
              "      <td>0.737333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000024</td>\n",
              "      <td>2.705215e-02</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>2.984571e-07</td>\n",
              "      <td>0.002926</td>\n",
              "      <td>3.863949e-03</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.011578</td>\n",
              "      <td>...</td>\n",
              "      <td>0.492707</td>\n",
              "      <td>0.689918</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>0.019046</td>\n",
              "      <td>0.017222</td>\n",
              "      <td>0.014865</td>\n",
              "      <td>0.549333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000036</td>\n",
              "      <td>2.802737e-02</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>5.201554e-07</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>2.547518e-03</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257772</td>\n",
              "      <td>0.243882</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018919</td>\n",
              "      <td>0.042667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000049</td>\n",
              "      <td>7.490901e-03</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>1.625190e-06</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>7.957064e-04</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.001628</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567210</td>\n",
              "      <td>0.460351</td>\n",
              "      <td>0.033595</td>\n",
              "      <td>0.022049</td>\n",
              "      <td>0.019542</td>\n",
              "      <td>0.019595</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175336</th>\n",
              "      <td>2.129647</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9.617817e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.499750e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Generic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175337</th>\n",
              "      <td>2.129660</td>\n",
              "      <td>8.429368e-03</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>1.675453e-06</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>9.065176e-04</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.002508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.819282</td>\n",
              "      <td>0.804002</td>\n",
              "      <td>0.026021</td>\n",
              "      <td>0.011434</td>\n",
              "      <td>0.021355</td>\n",
              "      <td>0.025676</td>\n",
              "      <td>0.029333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Shellcode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175338</th>\n",
              "      <td>2.129672</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9.617817e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.499750e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Generic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175339</th>\n",
              "      <td>2.129684</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9.617817e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.499750e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Generic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175340</th>\n",
              "      <td>2.129696</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9.617817e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.499750e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Generic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>175341 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id           dur    sbytes    dbytes      rate         sload  \\\n",
              "0       0.000000  2.024634e-03  0.000016  0.000012  0.000074  2.687726e-06   \n",
              "1       0.000012  1.083170e-02  0.000049  0.002866  0.000078  1.593605e-06   \n",
              "2       0.000024  2.705215e-02  0.000024  0.000900  0.000014  2.984571e-07   \n",
              "3       0.000036  2.802737e-02  0.000042  0.000053  0.000014  5.201554e-07   \n",
              "4       0.000049  7.490901e-03  0.000036  0.000018  0.000033  1.625190e-06   \n",
              "...          ...           ...       ...       ...       ...           ...   \n",
              "175336  2.129647  1.500000e-07  0.000006  0.000000  0.111111  9.617817e-03   \n",
              "175337  2.129660  8.429368e-03  0.000042  0.000024  0.000034  1.675453e-06   \n",
              "175338  2.129672  1.500000e-07  0.000006  0.000000  0.111111  9.617817e-03   \n",
              "175339  2.129684  1.500000e-07  0.000006  0.000000  0.111111  9.617817e-03   \n",
              "175340  2.129696  1.500000e-07  0.000006  0.000000  0.111111  9.617817e-03   \n",
              "\n",
              "           dload        sinpkt    dinpkt      sjit  ...     stcpb     dtcpb  \\\n",
              "0       0.000408  4.048592e-04  0.000145  0.000020  ...  0.144768  0.512828   \n",
              "1       0.024186  8.317781e-04  0.000267  0.000041  ...  0.330128  0.716525   \n",
              "2       0.002926  3.863949e-03  0.001779  0.011578  ...  0.492707  0.689918   \n",
              "3       0.000161  2.547518e-03  0.001563  0.000175  ...  0.257772  0.243882   \n",
              "4       0.000191  7.957064e-04  0.001310  0.001628  ...  0.567210  0.460351   \n",
              "...          ...           ...       ...       ...  ...       ...       ...   \n",
              "175336  0.000000  1.499750e-07  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "175337  0.000236  9.065176e-04  0.001160  0.002508  ...  0.819282  0.804002   \n",
              "175338  0.000000  1.499750e-07  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "175339  0.000000  1.499750e-07  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "175340  0.000000  1.499750e-07  0.000000  0.000000  ...  0.000000  0.000000   \n",
              "\n",
              "          tcprtt    synack    ackdat     smean     dmean  response_body_len  \\\n",
              "0       0.000000  0.000000  0.000000  0.012838  0.028667                0.0   \n",
              "1       0.000000  0.000000  0.000000  0.018919  0.737333                0.0   \n",
              "2       0.029281  0.019046  0.017222  0.014865  0.549333                0.0   \n",
              "3       0.000000  0.000000  0.000000  0.018919  0.042667                0.0   \n",
              "4       0.033595  0.022049  0.019542  0.019595  0.030000                0.0   \n",
              "...          ...       ...       ...       ...       ...                ...   \n",
              "175336  0.000000  0.000000  0.000000  0.022297  0.000000                0.0   \n",
              "175337  0.026021  0.011434  0.021355  0.025676  0.029333                0.0   \n",
              "175338  0.000000  0.000000  0.000000  0.022297  0.000000                0.0   \n",
              "175339  0.000000  0.000000  0.000000  0.022297  0.000000                0.0   \n",
              "175340  0.000000  0.000000  0.000000  0.022297  0.000000                0.0   \n",
              "\n",
              "        label  attack_cat  \n",
              "0         0.0      Normal  \n",
              "1         0.0      Normal  \n",
              "2         0.0      Normal  \n",
              "3         0.0      Normal  \n",
              "4         0.0      Normal  \n",
              "...       ...         ...  \n",
              "175336    1.0     Generic  \n",
              "175337    1.0   Shellcode  \n",
              "175338    1.0     Generic  \n",
              "175339    1.0     Generic  \n",
              "175340    1.0     Generic  \n",
              "\n",
              "[175341 rows x 21 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2sxKEN9qTNk2",
      "metadata": {
        "id": "2sxKEN9qTNk2"
      },
      "outputs": [],
      "source": [
        "class MauricioDataset(Dataset):\n",
        "    \"\"\"\n",
        "    it is a dataset where you need to define __getitem__ method\n",
        "    dataloader uses this method to sample from the dataset\n",
        "    i also make all the labels to be between 0 and 1\n",
        "\n",
        "    this dataset just performs a role of giving a row by its index\n",
        "    dataloader handles the shuffling\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, label_col: str):\n",
        "        self.df = df.copy().drop([label_col], axis=1)\n",
        "        scaler = MinMaxScaler() # i used minmax because the labels are scaled between 0 and 1, so it's easier\n",
        "        self.df = pd.DataFrame(scaler.fit_transform(self.df), index=self.df.index, columns=self.df.columns)\n",
        "        self.labels = df[label_col]\n",
        "        # unique_labels_num = list(self.labels.unique())\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        self.labels = le.fit_transform(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return one X and corresponding to it Y\n",
        "        return self.df.iloc[[idx]].values, torch.tensor([self.labels[idx]]) # data and its label (a word!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea90a217",
      "metadata": {
        "id": "ea90a217"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "dataset = MauricioDataset(trainset, 'attack_cat')\n",
        "# train test split - we dont need it here\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc1a49e",
      "metadata": {
        "id": "cbc1a49e"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, output_dim, noise_dim=32):\n",
        "        super(Generator, self).__init__()\n",
        "        input_out_feats = 64\n",
        "        emb_dim = 5\n",
        "\n",
        "        self.input_layer = nn.Sequential(nn.Linear(in_features=noise_dim, out_features=input_out_feats), nn.Flatten())\n",
        "        self.emb_y = nn.Sequential(nn.Embedding(num_embeddings=10,embedding_dim=emb_dim), nn.Flatten())\n",
        "        self.model = nn.Sequential( nn.Linear(input_out_feats + emb_dim, 128), nn.ReLU(),\n",
        "                                    nn.Linear(128, 256), nn.ReLU(),\n",
        "                                    nn.Linear(256, 512), nn.ReLU(),\n",
        "                                    nn.Linear(512, output_dim), nn.Tanh())\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "         x = self.input_layer(x)\n",
        "         y = self.emb_y(y)\n",
        "         return self.model(torch.cat((x, y), dim=1))\n",
        "        \n",
        "        \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        emb_dim = 5\n",
        "        self.emb_y = nn.Sequential(nn.Embedding(num_embeddings=10,embedding_dim=emb_dim), nn.Flatten())\n",
        "        self.model = nn.Sequential(nn.Linear(input_size + emb_dim, 512), nn.LeakyReLU(),\n",
        "                                   nn.Linear(512, 256), nn.LeakyReLU(),\n",
        "                                   nn.Linear(256, 128), nn.LeakyReLU(),\n",
        "                                   nn.Linear(128, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        y = self.emb_y(y)\n",
        "        if len(x.shape) > 2:\n",
        "            x = nn.Flatten()(x)\n",
        "        return self.model(torch.cat((x, y),dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6890e80",
      "metadata": {
        "id": "c6890e80",
        "outputId": "6e567432-a91a-49a4-e0b4-4403fcc3671f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 32])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def gen_noise(BATCH_SIZE, z_noise):\n",
        "    return torch.tensor(np.random.uniform(0., 1., size=[BATCH_SIZE, z_noise]))\n",
        "\n",
        "gen_noise(64, 32).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdeae32",
      "metadata": {
        "id": "afdeae32",
        "outputId": "eca44a14-600a-483a-a95d-4e1d8a4ceef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50882982",
      "metadata": {
        "id": "50882982"
      },
      "outputs": [],
      "source": [
        "n_epochs = 2000\n",
        "max_len = 20\n",
        "label_size = 10\n",
        "learning_rate = 0.0005\n",
        "\n",
        "D = Discriminator(input_size=max_len).to(device).float()\n",
        "G = Generator(output_dim=max_len).to(device).float()\n",
        "\n",
        "d_optimizer = optim.SGD(D.parameters(), learning_rate)\n",
        "g_optimizer = optim.SGD(G.parameters(), learning_rate)\n",
        "\n",
        "loss = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88a0d88",
      "metadata": {
        "id": "d88a0d88",
        "outputId": "7420db6c-7239-4deb-9469-0cef5875bf5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.24726512622592237 2.0175156043625173\n",
            "1 0.2484313202312167 1.9946281879352374\n",
            "2 0.2493674374572226 2.0003160092730545\n",
            "3 0.25000819760848353 2.025866682384841\n",
            "4 0.24034310434691458 2.036946446862228\n",
            "5 0.23672593381504992 2.0336705928271517\n",
            "6 0.2145401715977381 2.1679012256313817\n",
            "7 0.2404364179674982 2.141012227739183\n",
            "8 0.19719347192052736 2.286538260257448\n",
            "9 0.23953688955209673 2.087350787493526\n",
            "10 0.22566365662087362 2.166289823236955\n",
            "11 0.23491646444445263 2.220752987946915\n",
            "12 0.23613684092000892 2.207893104797945\n",
            "13 0.23540256734977985 2.235073456497252\n",
            "14 0.2305468415398249 2.3027787304962777\n",
            "15 0.23060384984845136 2.2648498753366724\n",
            "16 0.2313720731305112 2.3554133608775043\n",
            "17 0.2276484904402337 2.28818440474396\n",
            "18 0.22608542509527638 2.2993203579545205\n",
            "19 0.22926651780286436 2.352396287962586\n",
            "20 0.22631046661461124 2.3324904284247343\n",
            "21 0.22661802028399974 2.328947296224152\n",
            "22 0.22659485740804375 2.3878127646112515\n",
            "23 0.2254151583853443 2.3680482021942852\n",
            "24 0.22921498667327092 2.3346171591367173\n",
            "25 0.2208898803842012 2.461793549694874\n",
            "26 0.22358798287797307 2.357487644044274\n",
            "27 0.22922666805968686 2.3907896466611147\n",
            "28 0.22915791228853896 2.394482603154694\n",
            "29 0.22902914911833003 2.4093785501008456\n",
            "30 0.224434398769359 2.4209342822316646\n",
            "31 0.22893907577031136 2.3969562125984654\n",
            "32 0.2219380450378495 2.3920134157637603\n",
            "33 0.22483526320308214 2.432681549021838\n",
            "34 0.22635271502760113 2.398833616247073\n",
            "35 0.22780349527950605 2.4476835593279933\n",
            "36 0.22709131005044858 2.4098393690901263\n",
            "37 0.22816150331895793 2.4445699709199635\n",
            "38 0.22590336358264482 2.424939500784985\n",
            "39 0.2262921183258527 2.4275775094796117\n",
            "40 0.2262728479958804 2.437657442100311\n",
            "41 0.22435065026324053 2.440109467617646\n",
            "42 0.22905201834523659 2.4346143967626994\n",
            "43 0.22380535377545824 2.431162778363443\n",
            "44 0.2253216542129754 2.424864718183574\n",
            "45 0.22542635415149143 2.442956938713928\n",
            "46 0.2283625358058079 2.4604330995405443\n",
            "47 0.22717555664468145 2.469374698947414\n",
            "48 0.22843228987364525 2.4792324814418243\n",
            "49 0.22549745668171908 2.4673486302837055\n",
            "50 0.23087527073375175 2.4683155656602485\n",
            "51 0.2276422924725672 2.472092907684587\n",
            "52 0.22628471963281394 2.473705873333648\n",
            "53 0.22680661341821423 2.506865967673425\n",
            "54 0.22912324065832104 2.4667291830338733\n",
            "55 0.22815119814052368 2.513953390981695\n",
            "56 0.22915601902996213 2.4871146630721737\n",
            "57 0.22801304202451492 2.50211280065228\n",
            "58 0.22793591017277207 2.497186851538544\n",
            "59 0.22711706798795964 2.4746781366980093\n",
            "60 0.2265097634637986 2.5137834543388333\n",
            "61 0.22861302058280533 2.4554196872384884\n",
            "62 0.22689232212805638 2.4765625420847726\n",
            "63 0.22780982080505202 2.4682571034223812\n",
            "64 0.22635957649090613 2.460328106005121\n",
            "65 0.22695023139495502 2.479038266890898\n",
            "66 0.22571975035045416 2.4761825998315175\n",
            "67 0.2280650083583955 2.4837594117569517\n",
            "68 0.22639327658511246 2.4571527908230273\n",
            "69 0.22725325427706097 2.4728883970210194\n",
            "70 0.22706802968011117 2.4507780080635104\n",
            "71 0.22536486759202476 2.459649099347009\n",
            "72 0.22639425559487905 2.4619011589750346\n",
            "73 0.22458577168233473 2.46572883698284\n",
            "74 0.22680845308591197 2.4472017709055733\n",
            "75 0.22668486911206742 2.460080739776973\n",
            "76 0.2250668241832156 2.451185521404628\n",
            "77 0.2252244021494411 2.4611182433450205\n",
            "78 0.2255294284961457 2.4522497225697637\n",
            "79 0.22167190736736148 2.500983618651727\n",
            "80 0.22801401784779682 2.428458181176549\n",
            "81 0.22805763976315319 2.463661290067907\n",
            "82 0.22472710281170175 2.4355076210309705\n",
            "83 0.225265969535333 2.4412966723760956\n",
            "84 0.22687333760838116 2.4806014240993126\n",
            "85 0.2270176757770137 2.4293208724788706\n",
            "86 0.2259699653191014 2.4459785228569064\n",
            "87 0.22645873328436217 2.4417233190862797\n",
            "88 0.2250584348410507 2.4454523778258364\n",
            "89 0.22629802466064552 2.458194268621145\n",
            "90 0.22591516177400275 2.4413589511651828\n",
            "91 0.22830563088412603 2.4409480893964157\n",
            "92 0.22587242502933527 2.4631539911356164\n",
            "93 0.22669497922549536 2.5017576443833844\n",
            "94 0.22762315493210292 2.4291651929333273\n",
            "95 0.2274970178887781 2.449546515848922\n",
            "96 0.22544828227621214 2.4497032372243295\n",
            "97 0.22739353568566903 2.464759342129086\n",
            "98 0.22750159865752165 2.470615870845077\n",
            "99 0.223075312256674 2.4617387870236835\n",
            "100 0.22784953586421153 2.4563674230597807\n",
            "101 0.2279611818527174 2.4769360594578886\n",
            "102 0.22607531793079516 2.467229272306985\n",
            "103 0.2286971111319855 2.4658122717129127\n",
            "104 0.2261590203082024 2.4868781066979073\n",
            "105 0.2264130411446558 2.4513766551462712\n",
            "106 0.22629982862233375 2.4575526033923563\n",
            "107 0.22584466330857708 2.4631851937500167\n",
            "108 0.22544371596831556 2.4698782400988493\n",
            "109 0.2255849624240936 2.463762448697958\n",
            "110 0.22785212810427924 2.495377272135733\n",
            "111 0.22490873498317798 2.4638259822436104\n",
            "112 0.2259305239789779 2.4683825585927415\n",
            "113 0.22422455920378676 2.506882330677928\n",
            "114 0.22798765298344115 2.449437525743274\n",
            "115 0.22448216459799333 2.4588628113733293\n",
            "116 0.22646663322882557 2.455857775630328\n",
            "117 0.2248409607304292 2.472320891372153\n",
            "118 0.2266958884744214 2.495046683640725\n",
            "119 0.22494148325122904 2.4577448612794535\n",
            "120 0.22519063706832762 2.4665496833216904\n",
            "121 0.2259921873304095 2.476922783288021\n",
            "122 0.22338587769870624 2.541778651218207\n",
            "123 0.22930799889295692 2.44680180983447\n",
            "124 0.2264248289480736 2.476541463175606\n",
            "125 0.22674335778617005 2.471430018752767\n",
            "126 0.2249684745379221 2.468818265208931\n",
            "127 0.2263380214629889 2.4959002781433415\n",
            "128 0.22794374159622713 2.458006086653536\n",
            "129 0.22557977606682902 2.4571949217590165\n",
            "130 0.22365328607556423 2.4807077983675256\n",
            "131 0.22622114547975883 2.447798961058004\n",
            "132 0.22533225253641512 2.5616059675928593\n",
            "133 0.22709474957100526 2.446211863824776\n",
            "134 0.22499224294446304 2.4650071241992815\n",
            "135 0.22520740353393703 2.465535602895876\n",
            "136 0.22670413207256962 2.4875727730627935\n",
            "137 0.2275004716616118 2.4873737274766152\n",
            "138 0.22560180756111353 2.4644221100799775\n",
            "139 0.22384622805457835 2.4651573824103847\n",
            "140 0.22577570671936958 2.5035161751425283\n",
            "141 0.2243043904889427 2.471364187044823\n",
            "142 0.22444670718644122 2.4513762547847455\n",
            "143 0.22274218799190054 2.4742484598842047\n",
            "144 0.22869543310596635 2.4598537075389784\n",
            "145 0.2266713639150604 2.4765971505994187\n",
            "146 0.22095506874088366 2.530798022335462\n",
            "147 0.2294340279897672 2.4486976225120283\n",
            "148 0.22616912671372086 2.4687903871061643\n",
            "149 0.2257236782548309 2.4591290090540148\n",
            "150 0.225424879457958 2.4757201892220957\n",
            "151 0.22638716348222404 2.4936685402693683\n",
            "152 0.22469777460926985 2.4657870506980744\n",
            "153 0.22521926930194508 2.464797089667194\n",
            "154 0.22542499152372822 2.497831792705352\n",
            "155 0.22302645804073726 2.469177238677931\n",
            "156 0.22448122793950523 2.451272475589675\n",
            "157 0.2280331635282811 2.5429106859916106\n",
            "158 0.22364904717028047 2.4943855294543513\n",
            "159 0.22421759734303917 2.4492866179991286\n",
            "160 0.22880596201722025 2.468518288472948\n",
            "161 0.2280010882226249 2.454156289760655\n",
            "162 0.2256640238255772 2.462761120329007\n",
            "163 0.22406240886955017 2.470878942758077\n",
            "164 0.22631485823362277 2.4742305501252644\n",
            "165 0.22779391917456548 2.4652098122503303\n",
            "166 0.22448524445133114 2.4577493342233483\n",
            "167 0.22293770206081182 2.4758987411926174\n",
            "168 0.22141389224915817 2.5626438513143253\n",
            "169 0.22745755327572903 2.4788423428083095\n",
            "170 0.22697035248158137 2.455750144288106\n",
            "171 0.22577005031505604 2.456847656014177\n",
            "172 0.22455141839352588 2.5016344047816323\n",
            "173 0.22527722451215954 2.461283901987328\n",
            "174 0.22491226155270314 2.5212355569398976\n",
            "175 0.22658183408284077 2.4757228657765484\n",
            "176 0.22539074242138196 2.4809034573345867\n",
            "177 0.22556078115342007 2.4730149396285297\n",
            "178 0.22339438356192634 2.48919739582305\n",
            "179 0.22529195220691975 2.464959538743062\n",
            "180 0.2241233172790259 2.496578851161441\n",
            "181 0.22547967778301536 2.477179870078946\n",
            "182 0.2274471725718137 2.517982829228901\n",
            "183 0.2257590199332586 2.469236097940205\n",
            "184 0.22667527184886843 2.453449976574021\n",
            "185 0.22389685375323376 2.480851888471221\n",
            "186 0.22310250598943066 2.4763103882966107\n",
            "187 0.2238886309715211 2.464149335031376\n",
            "188 0.2246084974131447 2.5836682657060877\n",
            "189 0.22970368413610925 2.435000738867705\n",
            "190 0.22640788472876022 2.4627347782277393\n",
            "191 0.22643775341693387 2.446577879355447\n",
            "192 0.22293818867758125 2.4732896578997883\n",
            "193 0.2250463526470016 2.506782445529386\n",
            "194 0.22525902369391307 2.47929043254437\n",
            "195 0.22555274047641138 2.4736468448980045\n",
            "196 0.22715872955475216 2.448734033534167\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "197 0.22486527035270656 2.464439471253711\n",
            "198 0.22415480942901334 2.4887459606256677\n",
            "199 0.22341471760422224 2.4581573967985566\n",
            "200 0.22370890960746198 2.513799294528101\n",
            "201 0.22857675511928777 2.4605641587525837\n",
            "202 0.22528341278711894 2.454482088100113\n",
            "203 0.2236095416891167 2.46463595930933\n",
            "204 0.2244947700881476 2.5065682809423326\n",
            "205 0.22518383822813745 2.4755937967849184\n",
            "206 0.22460208472275808 2.459091947942648\n",
            "207 0.22643394440319822 2.457130301425097\n",
            "208 0.22292379324627146 2.4807596644198355\n",
            "209 0.22982606609260683 2.4663268097822626\n",
            "210 0.22339265520682972 2.5434596786973636\n",
            "211 0.22378935779026285 2.458476298889884\n",
            "212 0.22604898427081516 2.460828904230524\n",
            "213 0.2252688222792156 2.478947877698516\n",
            "214 0.2211084461091651 2.509884279771689\n",
            "215 0.2281540215108109 2.462571039540957\n",
            "216 0.22619208821543083 2.559759874922306\n",
            "217 0.22436840240705996 2.470925900465222\n",
            "218 0.22537254658563882 2.4747640102128403\n",
            "219 0.22488687204234709 2.514445912967759\n",
            "220 0.22420719723632881 2.4597250404847353\n",
            "221 0.2246228454063877 2.4726018297542494\n",
            "222 0.22466314731753817 2.500229391766188\n",
            "223 0.22514660491334335 2.4857837942488272\n",
            "224 0.22241900135324125 2.531812741448683\n",
            "225 0.22928446352852633 2.4448191387286267\n",
            "226 0.22294887348221584 2.4754103644639485\n",
            "227 0.22511882930530544 2.457511553682769\n",
            "228 0.22459350541975598 2.5554266361574736\n",
            "229 0.22599924451919218 2.4363345838445896\n",
            "230 0.22580450466177882 2.4473717455545074\n",
            "231 0.22368626888627027 2.450547005210657\n",
            "232 0.2240474918570433 2.532272120656715\n",
            "233 0.222739811836051 2.4889588441299986\n",
            "234 0.22702989523903022 2.4526681510693917\n",
            "235 0.22349565428092172 2.450291588925648\n",
            "236 0.22372700847557878 2.4811606624078233\n",
            "237 0.22789819390276914 2.4772345941878773\n",
            "238 0.2240944488631622 2.498294882047603\n",
            "239 0.22496179353393625 2.4616528100581783\n",
            "240 0.2228483939326106 2.4738466227963034\n",
            "241 0.2249850601734214 2.515487561144317\n",
            "242 0.2257236429022315 2.46253616479841\n",
            "243 0.2243931046274272 2.462890374901506\n",
            "244 0.22339738819684435 2.5359982496842997\n",
            "245 0.22456262417102862 2.463747160060224\n",
            "246 0.22534799177205395 2.457796638527332\n",
            "247 0.22476803100614304 2.4430480561397308\n",
            "248 0.21653561732075632 2.508749716204025\n",
            "249 0.22931608891412963 2.4378948755805636\n",
            "250 0.22543055619783758 2.490870532870849\n",
            "251 0.22422059565133662 2.5103172929624376\n",
            "252 0.2249748949453953 2.4491610797715966\n",
            "253 0.22551045898191666 2.4690466965338675\n",
            "254 0.22175203970000495 2.4839825448500426\n",
            "255 0.2240570544281143 2.496580597030616\n",
            "256 0.22709124334283973 2.462688776048865\n",
            "257 0.22428573057936102 2.468292520910177\n",
            "258 0.21860014489659255 2.6150745783771736\n",
            "259 0.23050303783261944 2.439129200174501\n",
            "260 0.22415133007160426 2.4756921427802387\n",
            "261 0.2265264586390548 2.500471532993702\n",
            "262 0.22365764644385125 2.4899488878472598\n",
            "263 0.22453704358166893 2.4619253792251063\n",
            "264 0.22028921898541332 2.5052972511963376\n",
            "265 0.22726439438401078 2.5056404144174387\n",
            "266 0.2209988919146139 2.493195806376485\n",
            "267 0.2282264162069346 2.4650791813018347\n",
            "268 0.22649635250562827 2.458213393276624\n",
            "269 0.2239982472619688 2.4684635495323786\n",
            "270 0.22395081272925227 2.4832721592664346\n",
            "271 0.22178184986114502 2.4829607480792197\n",
            "272 0.22355219928625955 2.4499724067944575\n",
            "273 0.22107406725895534 2.55699234913519\n",
            "274 0.22721763031664385 2.4633467633097577\n",
            "275 0.22482105306296105 2.5027408490470187\n",
            "276 0.2229355001303586 2.460131211284531\n",
            "277 0.22149986544302425 2.589570568286427\n",
            "278 0.22990704987134386 2.4477840576720644\n",
            "279 0.22609843471859514 2.460514564072919\n",
            "280 0.22454449857120196 2.4590612253171287\n",
            "281 0.22518004828116756 2.4460041719498937\n",
            "282 0.22470672480750417 2.5088330904951732\n",
            "283 0.22597255075747266 2.448203128752404\n",
            "284 0.2225912421026367 2.4651147852047792\n",
            "285 0.22361422119253715 2.489285551741557\n",
            "286 0.2242898716132155 2.571381417904078\n",
            "287 0.22553966231470346 2.44762328531286\n",
            "288 0.22374850911309893 2.4653951165093977\n",
            "289 0.2241538476030897 2.448657847674416\n",
            "290 0.22841456560287654 2.5241782587386585\n",
            "291 0.22438030892151697 2.4664702530411504\n",
            "292 0.223219233638901 2.4598539566177973\n",
            "293 0.22645485291654363 2.5248729114769777\n",
            "294 0.22405799526379416 2.471202618968246\n",
            "295 0.22297783527946213 2.4661578006729554\n",
            "296 0.22524844692269713 2.4420529387045424\n",
            "297 0.22229574175195227 2.5009020781999056\n",
            "298 0.22315713538444987 2.5018090172837346\n",
            "299 0.2248440367136365 2.4831741349322614\n",
            "300 0.22522053484620982 2.4555680953881605\n",
            "301 0.2214390333064051 2.4865559867900418\n",
            "302 0.22648674676637998 2.5824325023505774\n",
            "303 0.2207488328056239 2.4762110120586436\n",
            "304 0.22587379677044475 2.4712342333459927\n",
            "305 0.22609858323775034 2.4473766399950114\n",
            "306 0.2253889188657096 2.451388166814718\n",
            "307 0.2196877095935226 2.5759723332955344\n",
            "308 0.2274309919616946 2.460620823581334\n",
            "309 0.2262566349067918 2.4752479773101688\n",
            "310 0.22495682894441982 2.450085433052265\n",
            "311 0.22451876267837143 2.4697477205730336\n",
            "312 0.2254852620163472 2.4811606351546884\n",
            "313 0.22335865184039017 2.469011922275668\n",
            "314 0.22371350870138934 2.465684922264301\n",
            "315 0.22293198732644737 2.5938605630008467\n",
            "316 0.2270980232842788 2.463367213635571\n",
            "317 0.22560661174299002 2.4533844483584675\n",
            "318 0.22524889496562084 2.464993617026654\n",
            "319 0.22266630258868864 2.474219911762196\n",
            "320 0.22521951584316338 2.4843444774161973\n",
            "321 0.22298010911439106 2.492358377710286\n",
            "322 0.22435185469072863 2.4491101805566813\n",
            "323 0.22410872324860856 2.50579661955915\n",
            "324 0.22249259975844557 2.483660521718614\n",
            "325 0.22493308861370592 2.548347924767905\n",
            "326 0.22657137018850024 2.4429741335879216\n",
            "327 0.223554511416941 2.45347456557569\n",
            "328 0.22641624443082378 2.5143671764370814\n",
            "329 0.22079148463337084 2.4714182860002176\n",
            "330 0.22457977656049083 2.4414930317672563\n",
            "331 0.2236336777638499 2.4817513092124073\n",
            "332 0.22143028108736218 2.5110066923346155\n",
            "333 0.2227642930418392 2.60754746615794\n",
            "334 0.2248455820602262 2.46564776873329\n",
            "335 0.22447085350427434 2.4719892312913068\n",
            "336 0.22456875558172007 2.4647421495725355\n",
            "337 0.22656877267624875 2.5224218146055706\n",
            "338 0.2235314504911147 2.5149672244607384\n",
            "339 0.2265681174191216 2.4605609805179793\n",
            "340 0.2248912197458058 2.441344975590891\n",
            "341 0.2239847146090508 2.4770634187324605\n",
            "342 0.22593868449677945 2.4633188716731214\n",
            "343 0.22380325705693632 2.4540517626802436\n",
            "344 0.22499880404971992 2.494577121957094\n",
            "345 0.22187077176422576 2.4749269767274753\n",
            "346 0.2244606498935916 2.4482650029158703\n",
            "347 0.22486600070658777 2.5629040577548845\n",
            "348 0.22163266426134814 2.456024824367897\n",
            "349 0.22582112512265912 2.4447802033617188\n",
            "350 0.22629555683832148 2.4842377189158653\n",
            "351 0.2209096078414754 2.462145869320325\n",
            "352 0.2248856256227842 2.5171931651109483\n",
            "353 0.2241875692784879 2.468432779631014\n",
            "354 0.22409510324545448 2.4918406230294687\n",
            "355 0.2254842181761009 2.5626915480078285\n",
            "356 0.22382820018134073 2.5396980705379884\n",
            "357 0.22553313695021773 2.4670198274252018\n",
            "358 0.2257245634892495 2.4428465243075537\n",
            "359 0.22417525686381579 2.4571350525549005\n",
            "360 0.22718218023400655 2.480601254925943\n",
            "361 0.22514435808821748 2.449109980885756\n",
            "362 0.22373783737305164 2.438778578215402\n",
            "363 0.22438537456894736 2.4568876765934897\n",
            "364 0.21209392166503852 2.6782142904646475\n",
            "365 0.22684800256373908 2.4619706066362967\n",
            "366 0.22470065066083963 2.4505139116551233\n",
            "367 0.22441338507884398 2.445013395920512\n",
            "368 0.22539379064161708 2.520583460249389\n",
            "369 0.22385946881970387 2.4681837573577976\n",
            "370 0.22385824359895468 2.4677719572098407\n",
            "371 0.22550334064738933 2.5330561445808706\n",
            "372 0.22097939353662183 2.474422084043037\n",
            "373 0.2230827865447171 2.465112958688573\n",
            "374 0.22566819779030273 2.5551021176956676\n",
            "375 0.2232066782113182 2.4739918033961747\n",
            "376 0.22285518432571952 2.45862759939434\n",
            "377 0.22559133853942945 2.5371511381113696\n",
            "378 0.2203317608736444 2.4657719024047138\n",
            "379 0.22514650538494205 2.4846166255500046\n",
            "380 0.22313930914617067 2.467896060847198\n",
            "381 0.2219823657608885 2.486851107462383\n",
            "382 0.2247811273597818 2.5636536215734704\n",
            "383 0.22467725330016475 2.514777708850789\n",
            "384 0.22622138660395313 2.4548451451825133\n",
            "385 0.2266148714287841 2.4421439615786538\n",
            "386 0.22422724343477105 2.435295134458349\n",
            "387 0.22238405872698336 2.474060384148202\n",
            "388 0.21602748771245892 2.624403435928084\n",
            "389 0.22888369340895678 2.4679770198988136\n",
            "390 0.22605523981536343 2.4572546533996915\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391 0.22408258078350435 2.4494416461576756\n",
            "392 0.22472346712535313 2.5079028980913654\n",
            "393 0.22467360569056974 2.4493344475284893\n",
            "394 0.22205623600799967 2.450963587820252\n",
            "395 0.22544254706723507 2.5358347961357555\n",
            "396 0.22366119376608223 2.4667743836554177\n",
            "397 0.22446722149686718 2.4947521692115817\n",
            "398 0.22288697531003604 2.4642326384087556\n",
            "399 0.2198140494073531 2.4903935863293163\n",
            "400 0.227841748553292 2.4951050380711237\n",
            "401 0.2200045995356321 2.488176091648001\n",
            "402 0.22471562374389004 2.4379411597459537\n",
            "403 0.22413918951853623 2.486220085046154\n",
            "404 0.2235330763341016 2.4694802023203897\n",
            "405 0.22391123470052218 2.5471485745665814\n",
            "406 0.22173196965506992 2.5209418902315583\n",
            "407 0.22710438916230832 2.481570520816287\n",
            "408 0.22505639091968352 2.4863215731239614\n",
            "409 0.22609083308124245 2.4717706691236185\n",
            "410 0.22430955487684365 2.5079015000240985\n",
            "411 0.22602876754909243 2.4683801721822047\n",
            "412 0.22149401132602528 2.576889527159197\n",
            "413 0.22914376795801739 2.4442460798364407\n",
            "414 0.2240594082496029 2.441609884865733\n",
            "415 0.22501615414607395 2.5038073771107436\n",
            "416 0.22452761286154135 2.456266186285538\n",
            "417 0.2254188927194657 2.4522131556300315\n",
            "418 0.22123626567434931 2.544622740515654\n",
            "419 0.22599879325502212 2.4539343349484968\n",
            "420 0.2222322078818574 2.473731133281536\n",
            "421 0.2216826592156342 2.5683920535477656\n",
            "422 0.22732853375572254 2.4404535158240406\n",
            "423 0.22359661473479833 2.43874775454935\n",
            "424 0.22466802430189972 2.488179047778828\n",
            "425 0.22537589821136017 2.450817079603394\n",
            "426 0.2207338516578704 2.480974324198199\n",
            "427 0.2253424386098448 2.4712430641031933\n",
            "428 0.22109313660285335 2.5869960146224704\n",
            "429 0.22577513291458506 2.4621890536733955\n",
            "430 0.2249731554781575 2.49499230371849\n",
            "431 0.22401050422601343 2.4528043681689033\n",
            "432 0.22406041576392358 2.496086116897559\n",
            "433 0.22291608537151877 2.5232152536534596\n",
            "434 0.22382043478184874 2.5150919342485967\n",
            "435 0.2260214280725267 2.4411335493876813\n",
            "436 0.22395832391678824 2.4733598567835093\n",
            "437 0.22350416375935356 2.5865701559357475\n",
            "438 0.2212519470202515 2.454558462785154\n",
            "439 0.22490546926508426 2.4344063233439326\n",
            "440 0.2227782498564264 2.4518320831132714\n",
            "441 0.22066177602735684 2.4450750096960348\n",
            "442 0.22823256415837845 2.427956912695527\n",
            "443 0.22125394471441606 2.538551285886097\n",
            "444 0.2246684975933695 2.4459350858098055\n",
            "445 0.22707430689275543 2.46931716350152\n",
            "446 0.2226208600666067 2.487668055954098\n",
            "447 0.22111330661650394 2.5844176735143827\n",
            "448 0.2268029913931946 2.4854542282098557\n",
            "449 0.22668209192263394 2.4537949671456083\n",
            "450 0.22290596879242366 2.4467875143602886\n",
            "451 0.22471649109343905 2.5850536517370175\n",
            "452 0.22514720797005747 2.4490760106877256\n",
            "453 0.2226475049128892 2.4574467811391663\n",
            "454 0.22431185861258632 2.4838055566903408\n",
            "455 0.22367286538447415 2.529383637486127\n",
            "456 0.2253466929319209 2.443186165200051\n",
            "457 0.2264254101223282 2.478070360320167\n",
            "458 0.22053523232485422 2.5502386499155736\n",
            "459 0.22451704533113107 2.466702583603688\n",
            "460 0.2241902719041793 2.460491008372922\n",
            "461 0.2202237206996414 2.6047611722308406\n",
            "462 0.2251362913457639 2.482698617326342\n",
            "463 0.22427042771831826 2.4646738233684937\n",
            "464 0.22472638038347706 2.5154581507479605\n",
            "465 0.2248479155519424 2.470603347751623\n",
            "466 0.22083144313996167 2.4257281340114067\n",
            "467 0.21991712404748143 2.496906129883756\n",
            "468 0.21685826993934104 2.4577057930766886\n",
            "469 0.22109302747444268 2.542369438923351\n",
            "470 0.226566127940294 2.4337925260211595\n",
            "471 0.2271930018720879 2.4403233154565327\n",
            "472 0.22152752534690578 2.5379396626960435\n",
            "473 0.22709734755833264 2.457476728811605\n",
            "474 0.2274060783731066 2.4543727430548303\n",
            "475 0.22492487880967638 2.498894807154061\n",
            "476 0.22081661636870165 2.625606847031862\n",
            "477 0.2256601297211128 2.5078046891033\n",
            "478 0.22390222773651128 2.4916659877237226\n",
            "479 0.22747033204112788 2.5352534875157833\n",
            "480 0.22561758606583113 2.4991216001391967\n",
            "481 0.2246006917712477 2.4474184244268606\n",
            "482 0.22234455492017244 2.4682312252732865\n",
            "483 0.2261353358973982 2.493099667236186\n",
            "484 0.21971976257640505 2.475743389611667\n",
            "485 0.22593594235867773 2.4184302460535503\n",
            "486 0.22379902077615724 2.432913495352257\n",
            "487 0.22166663901315506 2.5062619311442456\n",
            "488 0.22422431354899058 2.4758409219062534\n",
            "489 0.22509137530047824 2.4942822914286684\n",
            "490 0.22649585464346464 2.5063255129854194\n",
            "491 0.21945751254471427 2.6173667685240276\n",
            "492 0.22557242659049495 2.4725588884546448\n",
            "493 0.22408922814400534 2.542749381269304\n",
            "494 0.22347721534536377 2.453166846353937\n",
            "495 0.2253086657131534 2.5265781154528746\n",
            "496 0.22331301836676398 2.471127399389703\n",
            "497 0.22414116502318004 2.40583867020036\n",
            "498 0.22592769554507677 2.475400045629979\n",
            "499 0.22061664469737102 2.4691468985232743\n",
            "500 0.22004039192204342 2.522026089728342\n",
            "501 0.22843888807537768 2.4205314360362005\n",
            "502 0.2252649395070495 2.4826914828450275\n",
            "503 0.2239381304853719 2.47764496873734\n",
            "504 0.21682742403741942 2.560078760884227\n",
            "505 0.2290169982758132 2.505201936973198\n",
            "506 0.221861231568071 2.5981870872051127\n",
            "507 0.2273624403804865 2.4666026360695694\n",
            "508 0.2230138440223912 2.4734998973865716\n",
            "509 0.2251628631230554 2.504856542702967\n",
            "510 0.22271189001471406 2.4279342157473645\n",
            "511 0.22434233945923313 2.489637361714851\n",
            "512 0.21976203551953355 2.489081288014378\n",
            "513 0.2284469498585023 2.3928273653909913\n",
            "514 0.2232208291291727 2.3913280037847313\n",
            "515 0.22356781811912546 2.400136183246298\n",
            "516 0.21699378098336572 2.4649901625527937\n",
            "517 0.21740242068888982 2.529428099862895\n",
            "518 0.23143457515892307 2.4174422673637723\n",
            "519 0.2238196783045866 2.447920012807772\n",
            "520 0.22598444559780842 2.4877151764384697\n",
            "521 0.22172832855169733 2.5702211594507447\n",
            "522 0.22710323878918798 2.481686301031157\n",
            "523 0.22576108729556596 2.4707529751546273\n",
            "524 0.22522845241483042 2.5019512267357453\n",
            "525 0.22483269632163908 2.489988483870938\n",
            "526 0.22124374471894503 2.4912233361930904\n",
            "527 0.22368422847582986 2.5856929266619644\n",
            "528 0.2227764349390244 2.5195749862753956\n",
            "529 0.22544743890640895 2.4804474773525635\n",
            "530 0.22407262057969768 2.4988418524595293\n",
            "531 0.22242366217227413 2.4731563526215856\n",
            "532 0.2269447234576542 2.510640973622098\n",
            "533 0.2242355728536427 2.4972822280545626\n",
            "534 0.22249867985514607 2.4668836239522576\n",
            "535 0.22309766174475104 2.5499189767644714\n",
            "536 0.2247066720277303 2.4550469781896376\n",
            "537 0.22583702188072086 2.4493018880046176\n",
            "538 0.21578012531388602 2.617874265644079\n",
            "539 0.22734779818200768 2.4417549710993076\n",
            "540 0.22573270856708427 2.4659205523100836\n",
            "541 0.2199788042525203 2.4889497881172975\n",
            "542 0.2263644625712331 2.4816630856245525\n",
            "543 0.22361208402883395 2.459708428864902\n",
            "544 0.2226011960841078 2.587885456174195\n",
            "545 0.22728620968979216 2.4485704173010947\n",
            "546 0.22264226979661508 2.431223751041418\n",
            "547 0.2245864639490333 2.526674994691905\n",
            "548 0.2230227862833818 2.4482955378284723\n",
            "549 0.22532979216467167 2.4641438063529564\n",
            "550 0.22083588604978974 2.587927100817857\n",
            "551 0.2255837021751871 2.4675709177880414\n",
            "552 0.2242405081506743 2.514686980477388\n",
            "553 0.22109707771665757 2.4773032813569063\n",
            "554 0.22624451935893825 2.4715030115462757\n",
            "555 0.22038664495569552 2.390718697389214\n",
            "556 0.22050652542900334 2.5125039369099618\n",
            "557 0.21956957617707978 2.526631101293846\n",
            "558 0.22579072375273446 2.411960179383054\n",
            "559 0.22286691552558174 2.422243109016359\n",
            "560 0.22617411505123874 2.386264890386009\n",
            "561 0.2244268390603514 2.445941066760483\n",
            "562 0.21956482688385073 2.6202840345272937\n",
            "563 0.22640552152741938 2.519075775406038\n",
            "564 0.22673292058298414 2.494761201864453\n",
            "565 0.22488159927920456 2.4644988022547674\n",
            "566 0.22517498051907003 2.490864885353931\n",
            "567 0.2250217616986802 2.5111943602005766\n",
            "568 0.22193904762760291 2.4723096145997707\n",
            "569 0.22208980301722397 2.58070865259571\n",
            "570 0.22711485375188187 2.4883009137855163\n",
            "571 0.2220784361716099 2.451549594454224\n",
            "572 0.22305793157586043 2.4973433879819664\n",
            "573 0.2265264152275459 2.466547934671583\n",
            "574 0.22300315208003088 2.5027321982161252\n",
            "575 0.2206927816358918 2.456237074375894\n",
            "576 0.2247175143258197 2.583137083572696\n",
            "577 0.2265338020147191 2.460604065128285\n",
            "578 0.22303231290511172 2.452965383766969\n",
            "579 0.222249920003429 2.519847764197627\n",
            "580 0.22267775973348558 2.5706522138456163\n",
            "581 0.23022244407174006 2.4353093144311506\n",
            "582 0.22246891327352955 2.486949795883143\n",
            "583 0.2237012800507097 2.5259291942160385\n",
            "584 0.22427272373118073 2.440530599832164\n",
            "585 0.22397686636675015 2.477657877453378\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "586 0.21729555939835485 2.6925505651841823\n",
            "587 0.22613218783011332 2.428403960879817\n",
            "588 0.2269446123652102 2.406722967398482\n",
            "589 0.22513505228987749 2.3749804810083486\n",
            "590 0.22532103952269533 2.3977069885326583\n",
            "591 0.21997724611364294 2.429118829974861\n",
            "592 0.22457389016729862 2.3959955052120687\n",
            "593 0.22214797030247946 2.4541997710245766\n",
            "594 0.22527750743144592 2.4275888712558182\n",
            "595 0.2194179072628496 2.468670586961239\n",
            "596 0.22505223207048272 2.4843321413867767\n",
            "597 0.22405254324640309 2.472921025326612\n",
            "598 0.2226756311964655 2.513074804613045\n",
            "599 0.22369620703380919 2.475997288527422\n",
            "600 0.22744752130649323 2.4720084537428977\n",
            "601 0.22424358106326908 2.480956696240379\n",
            "602 0.2235770660277288 2.5086727038512517\n",
            "603 0.22299925847961038 2.510069116092581\n",
            "604 0.22527677171847682 2.5936264587598123\n",
            "605 0.2250064337339316 2.4981613720000997\n",
            "606 0.22622234528974092 2.4846709112913388\n",
            "607 0.22366109369887535 2.4972727860484856\n",
            "608 0.22087952156274537 2.486382266040167\n",
            "609 0.22439847484485914 2.564561376675477\n",
            "610 0.22680446877496238 2.4589380572780293\n",
            "611 0.22276823415564342 2.488708859468878\n",
            "612 0.2259115431324505 2.4929667729797482\n",
            "613 0.2183558790369501 2.493335783203505\n",
            "614 0.22622929282289456 2.526060911396614\n",
            "615 0.2227416725924838 2.445591813483468\n",
            "616 0.2234047290986378 2.5354448482741834\n",
            "617 0.22458132916647205 2.4531880023876207\n",
            "618 0.22417324874115582 2.4470394995312668\n",
            "619 0.22430703784360856 2.4721356810158928\n",
            "620 0.22106596237138307 2.5146271198014634\n",
            "621 0.22672435897366255 2.425983984214522\n",
            "622 0.2245026300740928 2.500270587567408\n",
            "623 0.22356157013523634 2.572311102509684\n",
            "624 0.2250646377680461 2.4689770803481204\n",
            "625 0.22500664712991536 2.5409647178130794\n",
            "626 0.22554031708516747 2.4751735868201843\n",
            "627 0.22431401611185556 2.504670557486325\n",
            "628 0.22285247831076707 2.4888078484157012\n",
            "629 0.2248217764181631 2.466828921256503\n",
            "630 0.22078854849049315 2.460000983478493\n",
            "631 0.21579372678328634 2.626629729849369\n",
            "632 0.2268193498670962 2.5020468613222637\n",
            "633 0.22498289092471402 2.4676100432780075\n",
            "634 0.22270360326576938 2.5336164506746117\n",
            "635 0.22697321570610185 2.477551020137259\n",
            "636 0.2231120521993511 2.420051346855994\n",
            "637 0.22419242213826343 2.431051887212608\n",
            "638 0.21901969481288552 2.4105467212329943\n",
            "639 0.21268185760541986 2.4541036913962238\n",
            "640 0.21044464298693424 2.443640706513199\n",
            "641 0.20982631125587328 2.4721893949975864\n",
            "642 0.22487889560487 2.4263702345487674\n",
            "643 0.2135798652507191 2.5456155321646254\n",
            "644 0.23150351120978827 2.4580733654658493\n",
            "645 0.22669278150120198 2.41702097394789\n",
            "646 0.22694271920512477 2.462655440644936\n",
            "647 0.2288935679832291 2.4752796189410504\n",
            "648 0.22508137241515735 2.559550544707623\n",
            "649 0.22643046312231113 2.491964529313344\n",
            "650 0.22550281236878078 2.479148038623863\n",
            "651 0.22601238088349163 2.5809905427054707\n",
            "652 0.2274573537728186 2.5093386049033324\n",
            "653 0.22451743047864034 2.5246160504050423\n",
            "654 0.22569530313576733 2.56115110647252\n",
            "655 0.22523196002766838 2.490364345199027\n",
            "656 0.2270433421233672 2.488391889939968\n",
            "657 0.22629838491881246 2.505667421346875\n",
            "658 0.22233970378689225 2.5146734466078122\n",
            "659 0.2235225910381711 2.436992691056354\n",
            "660 0.22601149964666292 2.4722849616366633\n",
            "661 0.22412431292803625 2.4665146522907597\n",
            "662 0.22420247215458988 2.5953570402243273\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m generator_discriminator_loss \u001b[38;5;241m=\u001b[39m loss(discriminator_output_for_generated_data, torch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     27\u001b[0m discriminator_loss \u001b[38;5;241m=\u001b[39m (true_discriminator_loss \u001b[38;5;241m+\u001b[39m generator_discriminator_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mdiscriminator_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m D_loss\u001b[38;5;241m.\u001b[39mappend(discriminator_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dloss = list()\n",
        "gloss = list()\n",
        "\n",
        "from statistics import mean\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  D_loss = []\n",
        "  G_loss = []\n",
        "  G.train()\n",
        "  D.train()\n",
        "  for true_data, labels in dataloader:\n",
        "    true_data = true_data.to(device).float()\n",
        "    labels = labels.to(device)\n",
        "    BATCH_SIZE = true_data.shape[0]\n",
        "\n",
        "    noise = gen_noise(64, 32).to(device)    \n",
        "    fake_labels = torch.randint(0, 4, (BATCH_SIZE,)).to(device)\n",
        "    generated_data = G(noise.float(), fake_labels) \n",
        "    true_labels = torch.ones(BATCH_SIZE).to(device)\n",
        "\n",
        "    d_optimizer.zero_grad()\n",
        "    discriminator_output_for_true_data = D(true_data, labels).view(BATCH_SIZE)\n",
        "    true_discriminator_loss = loss(discriminator_output_for_true_data, true_labels)\n",
        "\n",
        "    discriminator_output_for_generated_data = D(generated_data.detach(), fake_labels).view(BATCH_SIZE)\n",
        "    generator_discriminator_loss = loss(discriminator_output_for_generated_data, torch.zeros(BATCH_SIZE).to(device))\n",
        "    discriminator_loss = (true_discriminator_loss + generator_discriminator_loss)/2\n",
        "            \n",
        "    discriminator_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    D_loss.append(discriminator_loss.data.item())\n",
        "\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "    generated_data = G(noise.float(), fake_labels) \n",
        "    discriminator_output_on_generated_data = D(generated_data, fake_labels).view(BATCH_SIZE)\n",
        "    generator_loss = loss(discriminator_output_on_generated_data, true_labels)\n",
        "    generator_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    G_loss.append(generator_loss.data.item())\n",
        "  \n",
        "  dloss.append(D_loss)\n",
        "  gloss.append(G_loss)\n",
        "\n",
        "  print(epoch, mean(D_loss), mean(G_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e836767",
      "metadata": {
        "id": "2e836767"
      },
      "source": [
        "### Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201c632e",
      "metadata": {
        "id": "201c632e"
      },
      "outputs": [],
      "source": [
        "finald = list()\n",
        "finalg = list()\n",
        "\n",
        "for i in range(660):\n",
        "    finald.append(np.mean(dloss[i]))\n",
        "    finalg.append(np.mean(gloss[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81db5679",
      "metadata": {
        "id": "81db5679",
        "outputId": "fdf65ce5-3c85-49b4-d6a9-0ca4334c9358"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(finald, label=\"Loss D\")\n",
        "plt.plot(finalg, label=\"Loss G\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.title(\"Loss cGan\", fontsize=30) \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a7c165",
      "metadata": {
        "id": "55a7c165"
      },
      "source": [
        "### DATA LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc0cd54",
      "metadata": {
        "id": "4fc0cd54"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "trainset['attack_cat'] = le.fit_transform(trainset['attack_cat'].values)\n",
        "testset['attack_cat'] = le.transform(testset['attack_cat'].values)\n",
        "\n",
        "mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "names = dict((v,k) for k,v in mapping.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2880c685",
      "metadata": {
        "id": "2880c685"
      },
      "outputs": [],
      "source": [
        "trainset.to_csv(r'train', index=False)\n",
        "testset.to_csv(r'test', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3734d4c4",
      "metadata": {
        "id": "3734d4c4"
      },
      "source": [
        "### CREATION NEW DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826eea4f",
      "metadata": {
        "id": "826eea4f"
      },
      "outputs": [],
      "source": [
        "columnas = list(trainset.columns)\n",
        "columnas.remove('attack_cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc1342b",
      "metadata": {
        "id": "abc1342b"
      },
      "outputs": [],
      "source": [
        "num_rows_to_generate = 4956\n",
        "label_to_generate = 9\n",
        "noise = gen_noise(num_rows_to_generate, 32).to(device).float()\n",
        "labels = torch.tensor([[label_to_generate] for _ in range(num_rows_to_generate)]).to(device)\n",
        "fake_rows = G(noise, labels).tolist()\n",
        "class9 = pd.DataFrame(fake_rows, columns=columnas)\n",
        "class9['attack_cat'] = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fdaca5",
      "metadata": {
        "id": "43fdaca5"
      },
      "outputs": [],
      "source": [
        "class9.to_csv(r'class9', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00a4831",
      "metadata": {
        "id": "d00a4831"
      },
      "outputs": [],
      "source": [
        "num_rows_to_generate = 4622\n",
        "label_to_generate = 8\n",
        "noise = gen_noise(num_rows_to_generate, 32).to(device).float()\n",
        "labels = torch.tensor([[label_to_generate] for _ in range(num_rows_to_generate)]).to(device)\n",
        "fake_rows = G(noise, labels).tolist()\n",
        "class8 = pd.DataFrame(fake_rows, columns=columnas)\n",
        "class8['attack_cat'] = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8789869",
      "metadata": {
        "id": "f8789869"
      },
      "outputs": [],
      "source": [
        "class8.to_csv(r'class8', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1867a60",
      "metadata": {
        "id": "d1867a60"
      },
      "outputs": [],
      "source": [
        "num_rows_to_generate = 4417\n",
        "label_to_generate = 1\n",
        "noise = gen_noise(num_rows_to_generate, 32).to(device).float()\n",
        "labels = torch.tensor([[label_to_generate] for _ in range(num_rows_to_generate)]).to(device)\n",
        "fake_rows = G(noise, labels).tolist()\n",
        "class1 = pd.DataFrame(fake_rows, columns=columnas)\n",
        "class1['attack_cat'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b5ad93",
      "metadata": {
        "id": "c0b5ad93"
      },
      "outputs": [],
      "source": [
        "class1.to_csv(r'class1', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e39310",
      "metadata": {
        "id": "e2e39310"
      },
      "outputs": [],
      "source": [
        "num_rows_to_generate = 4323\n",
        "label_to_generate = 0\n",
        "noise = gen_noise(num_rows_to_generate, 32).to(device).float()\n",
        "labels = torch.tensor([[label_to_generate] for _ in range(num_rows_to_generate)]).to(device)\n",
        "fake_rows = G(noise, labels).tolist()\n",
        "class0 = pd.DataFrame(fake_rows, columns=columnas)\n",
        "class0['attack_cat'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79487e63",
      "metadata": {
        "id": "79487e63"
      },
      "outputs": [],
      "source": [
        "class0.to_csv(r'class0', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f3ca6f",
      "metadata": {
        "id": "09f3ca6f"
      },
      "outputs": [],
      "source": [
        "num_rows_to_generate = 1504\n",
        "label_to_generate = 7\n",
        "noise = gen_noise(num_rows_to_generate, 32).to(device).float()\n",
        "labels = torch.tensor([[label_to_generate] for _ in range(num_rows_to_generate)]).to(device)\n",
        "fake_rows = G(noise, labels).tolist()\n",
        "class7 = pd.DataFrame(fake_rows, columns=columnas)\n",
        "class7['attack_cat'] = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e92cd0",
      "metadata": {
        "id": "e7e92cd0"
      },
      "source": [
        "### TRAINSET AUGMENTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6bae51ac",
      "metadata": {
        "id": "6bae51ac"
      },
      "outputs": [],
      "source": [
        "testset  = pd.read_csv('test.csv')\n",
        "trainset  = pd.read_csv('train.csv')\n",
        "\n",
        "class9  = pd.read_csv('class9.csv')\n",
        "class8  = pd.read_csv('class8.csv')\n",
        "class1  = pd.read_csv('class1.csv')\n",
        "class0  = pd.read_csv('class0.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6e8b2ddd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e8b2ddd",
        "outputId": "c49a5882-9012-4f4a-9294-7dbf3c704415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82332, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "trainset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "df5b08d8",
      "metadata": {
        "id": "df5b08d8"
      },
      "outputs": [],
      "source": [
        "trainsetaug = pd.concat([trainset,class9,class8,class1,class0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bf50e85e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf50e85e",
        "outputId": "8274e001-0d7d-408b-ec1e-e14e143e2a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100650, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y_trainaug = trainsetaug[\"label\"].apply(np.round).abs()\n",
        "X_trainaug = trainsetaug.drop(\"label\", 1)\n",
        "trainsetaug.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790bfbaf",
      "metadata": {
        "id": "790bfbaf"
      },
      "source": [
        "### SPLIT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "74835710",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74835710",
        "outputId": "d118d411-5431-4231-ea1e-d3c5bedccc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "y_train = trainset[\"label\"]\n",
        "X_train = trainset.drop(\"label\", 1)\n",
        "\n",
        "y_test = testset[\"label\"]\n",
        "X_test = testset.drop(\"label\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8f130f",
      "metadata": {
        "id": "0d8f130f"
      },
      "source": [
        "# CLASSIFIERS "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b903677",
      "metadata": {
        "id": "6b903677"
      },
      "source": [
        "### RANDOM FOREST: NO DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fd6a33d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd6a33d5",
        "outputId": "0966d2c9-e76d-44c7-8bef-387225fd0471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.997 (+/-0.010) for {'n_estimators': 5}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'n_estimators' : [5]}\n",
        "\n",
        "model = GridSearchCV(estimator = RandomForestClassifier(), scoring='recall', param_grid = parameters)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "means = model.cv_results_['mean_test_score']\n",
        "stds = model.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1f90aaa4",
      "metadata": {
        "id": "1f90aaa4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_predclf = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_predclf)\n",
        "print(classification_report(y_test, y_predclf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8NXDrTplvVZ",
        "outputId": "950e0056-ace5-48ea-c6dd-8f7bee9e1d73"
      },
      "id": "R8NXDrTplvVZ",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      1.00      0.92     56000\n",
            "         1.0       1.00      0.92      0.96    119341\n",
            "\n",
            "    accuracy                           0.95    175341\n",
            "   macro avg       0.93      0.96      0.94    175341\n",
            "weighted avg       0.96      0.95      0.95    175341\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "e38b300f",
      "metadata": {
        "id": "e38b300f"
      },
      "outputs": [],
      "source": [
        "### RANDOM FOREST: DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "6ac4daae",
      "metadata": {
        "id": "6ac4daae"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_trainaug, y_trainaug)\n",
        "\n",
        "y_predclf = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_predclf)\n",
        "print(classification_report(y_test, y_predclf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl3u3h-xl142",
        "outputId": "ae1f5211-997c-4c51-a4ca-29e18fda3c5c"
      },
      "id": "wl3u3h-xl142",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96     56000\n",
            "         1.0       1.00      0.96      0.98    119341\n",
            "\n",
            "    accuracy                           0.97    175341\n",
            "   macro avg       0.96      0.98      0.97    175341\n",
            "weighted avg       0.97      0.97      0.97    175341\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a71526c",
      "metadata": {
        "id": "7a71526c"
      },
      "source": [
        "### CLASSICAL NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "37b9edfe",
      "metadata": {
        "id": "37b9edfe"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,inp,H,output):\n",
        "        super(Net,self).__init__()\n",
        "        self.linear1=nn.Linear(inp,H)\n",
        "        self.linear2=nn.Linear(H,output)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=torch.sigmoid(self.linear1(x))  \n",
        "        x=self.linear2(x)\n",
        "        return x\n",
        "    # def __init_weights(self,m):\n",
        "    #   if type(m) == nn.Linear:\n",
        "    #     torch.nn.init.xavier_uniform_(m.weight)\n",
        "    #     m.bias.data.fill_(0.01)\n",
        "input_dim,hidden_dim,output_dim = 42, 10, 1\n",
        "SimpleNN = Net(input_dim,hidden_dim,output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.002 \n",
        "num_epochs = 10\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size,shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "cW0DSxRCqyWd"
      },
      "id": "cW0DSxRCqyWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSE()\n",
        "optimizer = torch.optim.Adam(SimpleNN.parameters(),lr=learning_rate)\n",
        "for epoch in range(num_epochs): \n",
        "  epoch_loss = list() \n",
        "  for X_batch in train_dataset: \n",
        "    optimizer.zero_grad() \n",
        "    inputs, labels = X_batch\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    \n",
        "    output = SimpleNN(inputs) \n",
        "    loss = criterion(labels, output) \n",
        " \n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        " \n",
        "    epoch_loss.append(loss.item()) \n",
        "  print(f'[{epoch}/{num_epochs}]', f'loss: {np.array(epoch_loss).mean()}')"
      ],
      "metadata": {
        "id": "DtkTpcBCoztC"
      },
      "id": "DtkTpcBCoztC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  SimpleNN.eval()\n",
        "  X_test, y_test = testloader\n",
        "  y_pred = pred(SimpleNN(X_test)).to_numpy().detach()\n",
        "  classification_report(np.around(y_pred),y_test)"
      ],
      "metadata": {
        "id": "PjTC5SKdq5CY"
      },
      "id": "PjTC5SKdq5CY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXPLAINABLE BOOSTING MACHINE: NO DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "Jxdg6C63knoW"
      },
      "id": "Jxdg6C63knoW"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c8f83ccf",
      "metadata": {
        "id": "c8f83ccf"
      },
      "outputs": [],
      "source": [
        "ebm = ExplainableBoostingClassifier()\n",
        "ebm.fit(X_train, y_train)\n",
        "\n",
        "y = ebm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y)\n",
        "print(classification_report(y_test, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXvS_DQEl42o",
        "outputId": "151b17d3-e615-4b5c-f514-5e87f7ae2c9e"
      },
      "id": "wXvS_DQEl42o",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     56000\n",
            "         1.0       1.00      1.00      1.00    119341\n",
            "\n",
            "    accuracy                           1.00    175341\n",
            "   macro avg       1.00      1.00      1.00    175341\n",
            "weighted avg       1.00      1.00      1.00    175341\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### EXPLAINABLE BOOSTING MACHINE: DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "HZLJ5lBvk1aw"
      },
      "id": "HZLJ5lBvk1aw",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ebm = ExplainableBoostingClassifier()\n",
        "ebm.fit(X_trainaug, y_trainaug)\n",
        "\n",
        "y = ebm.predict(X_test)"
      ],
      "metadata": {
        "id": "yG8Dinamk1hj"
      },
      "id": "yG8Dinamk1hj",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y)\n",
        "print(classification_report(y_test, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCzrNdvIlojk",
        "outputId": "0ca4fb2d-07d4-43cd-facf-1cc82f54b0a5"
      },
      "id": "tCzrNdvIlojk",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     56000\n",
            "         1.0       1.00      1.00      1.00    119341\n",
            "\n",
            "    accuracy                           1.00    175341\n",
            "   macro avg       1.00      1.00      1.00    175341\n",
            "weighted avg       1.00      1.00      1.00    175341\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fe9nec6mnsNp"
      },
      "id": "fe9nec6mnsNp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}